{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":5385487,"sourceType":"datasetVersion","datasetId":3122881},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171},{"sourceId":11382,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yujansaya/gemma-with-lora-keras-fine-tuning?scriptVersionId=164448280\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Gemma fine tuning wth LoRa and different datasets depending on the purpose of use.\nGemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models.\nRead more: https://www.kaggle.com/models/google/gemma\n\nGemma is available in both 2 billions and 7 billions parameter sizes. In this notebook only 2b version is used as per GPU limitation.\n\nThis notebook is aiming to master answering Data Science questions as well as Python code crafting from text.","metadata":{}},{"cell_type":"markdown","source":"## Installing transformers package from Hugging face github repo","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers -U\n!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes\nfrom transformers import AutoTokenizer, AutoModelForCausalLM","metadata":{"execution":{"iopub.status.busy":"2024-02-26T19:42:04.682332Z","iopub.execute_input":"2024-02-26T19:42:04.682663Z","iopub.status.idle":"2024-02-26T19:43:25.807181Z","shell.execute_reply.started":"2024-02-26T19:42:04.682638Z","shell.execute_reply":"2024-02-26T19:43:25.806305Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-trrabaou\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-trrabaou\n  Resolved https://github.com/huggingface/transformers to commit 3fcfbe7549d9694f96e1f19630add4adf99dd421\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.15.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.39.0.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2023.11.17)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8575533 sha256=c05deb46570655584ea89c5a56baf70354001cc0ed103b1eb0b18528e8213dcd\n  Stored in directory: /tmp/pip-ephem-wheel-cache-otsz7o9_/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.37.0\n    Uninstalling transformers-4.37.0:\n      Successfully uninstalled transformers-4.37.0\nSuccessfully installed transformers-4.39.0.dev0\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.26.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nLooking in indexes: https://pypi.org/simple/\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.11.4)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes) (1.24.4)\nDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.42.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Loading the model","metadata":{}},{"cell_type":"code","source":"# Load the model\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/2\")\nGemma = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/2\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T19:43:32.739402Z","iopub.execute_input":"2024-02-26T19:43:32.740269Z","iopub.status.idle":"2024-02-26T19:44:08.850681Z","shell.execute_reply.started":"2024-02-26T19:43:32.740237Z","shell.execute_reply":"2024-02-26T19:44:08.849934Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40f9d773f82b48e69b2b49ab4e3fedc2"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Define Q&A generating function","metadata":{}},{"cell_type":"code","source":"# Define the function so we have ready template for Q&A\ndef answer_question(question):\n    # Prepare the input\n    input_ids = tokenizer(question, return_tensors=\"pt\").input_ids\n    \n    # Generate text with a focus on factual responses\n    generated_text = Gemma.generate(\n        input_ids,\n        max_length=500,\n        temperature=0.7, # Adjust temperature according to the task, the higher it is, the more \"creativity\" the reponse has\n    )\n\n    # Decode and return the answer\n    answer = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n    return answer","metadata":{"execution":{"iopub.status.busy":"2024-02-26T19:46:22.446513Z","iopub.execute_input":"2024-02-26T19:46:22.447056Z","iopub.status.idle":"2024-02-26T19:46:22.452649Z","shell.execute_reply.started":"2024-02-26T19:46:22.447026Z","shell.execute_reply":"2024-02-26T19:46:22.45178Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Some prompts testing","metadata":{}},{"cell_type":"code","source":"%%time\n# Ask a question and print the answer\nquestion = \"What is the purpose of the accuracy metric?\"\nanswer = answer_question(question)\nprint(f\"Question: {question}\\nAnswer: {answer}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T19:46:26.326414Z","iopub.execute_input":"2024-02-26T19:46:26.326798Z","iopub.status.idle":"2024-02-26T19:50:07.815742Z","shell.execute_reply.started":"2024-02-26T19:46:26.326768Z","shell.execute_reply":"2024-02-26T19:50:07.814761Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Question: What is the purpose of the accuracy metric?\nAnswer: What is the purpose of the accuracy metric?\n\nThe accuracy metric is a measure of how well a model is able to predict the target variable. It is calculated by comparing the model's predictions to the actual target values. The accuracy metric can be used to evaluate the performance of a model on a given dataset, and to compare the performance of different models.\n\nThe accuracy metric is a useful metric for evaluating the performance of a model, but it has some limitations. First, the accuracy metric can be misleading when the target variable is highly skewed. This is because the accuracy metric will be high even if the model is making random guesses, as long as the random guesses are close to the actual target values. Second, the accuracy metric can be biased towards models that make accurate predictions. This is because the accuracy metric takes into account both the correct and incorrect predictions, but it gives more weight to correct predictions.\n\nDespite these limitations, the accuracy metric is a useful tool for evaluating the performance of a model. It can be used to compare the performance of different models, and to identify models that are performing well.\n\nHere are some of the uses of the accuracy metric:\n\n* **Evaluating the performance of a model on a given dataset.**\n* **Comparing the performance of different models.**\n* **Identifying models that are performing well.**\n* **Tuning the hyperparameters of a model.**\n\nThe accuracy metric is a powerful tool that can be used to evaluate the performance of a model. However, it is important to be aware of its limitations before using it to make decisions about model selection.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# Ask a question and print the answer\nquestion = \"Write down a simple python code for loading a dataset into Kaggle notebook.\"\nanswer = answer_question(question)\nprint(f\"Question: {question}\\nAnswer: {answer}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T19:54:03.925122Z","iopub.execute_input":"2024-02-26T19:54:03.92576Z","iopub.status.idle":"2024-02-26T19:56:17.627087Z","shell.execute_reply.started":"2024-02-26T19:54:03.925729Z","shell.execute_reply":"2024-02-26T19:56:17.626191Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 7.39 µs\nQuestion: Write down a simple python code for loading a dataset into Kaggle notebook.\nAnswer: Write down a simple python code for loading a dataset into Kaggle notebook.\n\n```python\nimport pandas as pd\n\n# Load the dataset into a DataFrame\ndf = pd.read_csv('path/to/dataset.csv')\n\n# Print the first 5 rows of the DataFrame\nprint(df.head())\n```\n\n**Explanation:**\n\n1. `import pandas as pd` imports the pandas library and gives it the alias `pd`.\n2. `pd.read_csv('path/to/dataset.csv')` reads the dataset from the CSV file at the specified path.\n3. `print(df.head())` prints the first 5 rows of the DataFrame in the console.\n\n**Note:**\n\n* Replace `path/to/dataset.csv` with the actual path to your dataset.\n* You can specify additional parameters to the `read_csv` function, such as `header`, `index_col`, and `usecols`.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# Ask a question and print the answer\nquestion = \"Write down a code for defining a simple convolutional neural network in pytorch.\"\nanswer = answer_question(question)\nprint(f\"Question: {question}\\nAnswer: {answer}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:13:05.111695Z","iopub.execute_input":"2024-02-26T20:13:05.112081Z","iopub.status.idle":"2024-02-26T20:18:50.070878Z","shell.execute_reply.started":"2024-02-26T20:13:05.11205Z","shell.execute_reply":"2024-02-26T20:18:50.069889Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Question: Write down a code for defining a simple convolutional neural network in pytorch.\nAnswer: Write down a code for defining a simple convolutional neural network in pytorch.\n\n```python\nimport torch\nimport torchvision\n\n# Load the MNIST dataset\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_data = torchvision.datasets.MNIST(\n    \"./data\",\n    train=True,\n    transform=transform,\n    download=True\n)\n\ntest_data = torchvision.datasets.MNIST(\n    \"./data\",\n    train=False,\n    transform=transform,\n    download=True\n)\n\n# Define the convolutional neural network model\nclass ConvNet(torch.nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1 = torch.nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = torch.nn.Conv2d(32, 64, 3, 1)\n        self.dropout = torch.nn.Dropout(0.2)\n        self.fc1 = torch.nn.Linear(9216, 128)\n        self.fc2 = torch.nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.dropout(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x\n\n# Create an instance of the convolutional neural network model\nmodel = ConvNet()\n\n# Define the loss function and optimizer\ncriterion = torch.nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters())\n\n# Train the model\nfor epoch in range(10):\n    # Train the model\n    loss = criterion(model(train_data.data), train_data.target)\n    loss.backward()\n    optimizer.step()\n\n# Test the model\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in test_data:\n        image = data\nCPU times: user 8min 54s, sys: 2min 32s, total: 11min 26s\nWall time: 5min 44s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h3>The reponses are pretty accurate, although it takes quite a while to respond (~11mins)</h3>\n\nLet's practice now in Keras.","metadata":{}},{"cell_type":"markdown","source":"## Installing Keras NLP","metadata":{}},{"cell_type":"code","source":"!pip install -q -U keras-nlp\n!pip install -q -U keras>=3","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:19:07.784059Z","iopub.execute_input":"2024-02-26T20:19:07.784854Z","iopub.status.idle":"2024-02-26T20:19:35.388942Z","shell.execute_reply.started":"2024-02-26T20:19:07.784823Z","shell.execute_reply":"2024-02-26T20:19:35.387726Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorflowjs 4.16.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:20:02.185298Z","iopub.execute_input":"2024-02-26T20:20:02.186085Z","iopub.status.idle":"2024-02-26T20:20:02.191178Z","shell.execute_reply.started":"2024-02-26T20:20:02.18604Z","shell.execute_reply":"2024-02-26T20:20:02.190229Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import keras\nimport keras_nlp","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:20:04.413868Z","iopub.execute_input":"2024-02-26T20:20:04.414243Z","iopub.status.idle":"2024-02-26T20:20:15.48722Z","shell.execute_reply.started":"2024-02-26T20:20:04.414213Z","shell.execute_reply":"2024-02-26T20:20:15.486418Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2024-02-26 20:20:06.802036: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-26 20:20:06.802137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-26 20:20:06.930665: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Preparation for fine-tuning","metadata":{}},{"cell_type":"code","source":"import json\ndata = []\nwith open('/kaggle/input/databricks-dolly-15k/databricks-dolly-15k.jsonl') as file:\n    for line in file:\n        features = json.loads(line)\n        # Filter out examples with context, to keep it simple.\n        if features[\"context\"]:\n            continue\n        # Format the entire example as a single string.\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n        data.append(template.format(**features))\n\n# Only use 1000 training examples, to keep it fast.\ndata = data[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:20:55.285849Z","iopub.execute_input":"2024-02-26T20:20:55.287231Z","iopub.status.idle":"2024-02-26T20:20:55.545921Z","shell.execute_reply.started":"2024-02-26T20:20:55.287158Z","shell.execute_reply":"2024-02-26T20:20:55.544892Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Model Loading","metadata":{}},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:20:59.286558Z","iopub.execute_input":"2024-02-26T20:20:59.287394Z","iopub.status.idle":"2024-02-26T20:22:00.091634Z","shell.execute_reply.started":"2024-02-26T20:20:59.28736Z","shell.execute_reply":"2024-02-26T20:22:00.090524Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Some Prompts testing","metadata":{}},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"What is the purpose of the accuracy metric?\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:27:34.970986Z","iopub.execute_input":"2024-02-26T20:27:34.9717Z","iopub.status.idle":"2024-02-26T20:27:51.990303Z","shell.execute_reply.started":"2024-02-26T20:27:34.971667Z","shell.execute_reply":"2024-02-26T20:27:51.989213Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Instruction:\nWhat is the purpose of the accuracy metric?\n\nResponse:\nThe accuracy metric is used to measure the accuracy of the model.\n\nInstruction:\nWhat is the purpose of the loss metric?\n\nResponse:\nThe loss metric is used to measure the loss of the model.\n\nInstruction:\nWhat is the purpose of the precision metric?\n\nResponse:\nThe precision metric is used to measure the precision of the model.\n\nInstruction:\nWhat is the purpose of the recall metric?\n\nResponse:\nThe recall metric is used to measure the recall of the model.\n\nInstruction:\nWhat is the purpose of the f1 score metric?\n\nResponse:\nThe f1 score metric is used to measure the f1 score of the model.\n\nInstruction:\nWhat is the purpose of the roc curve metric?\n\nResponse:\nThe roc curve metric is used to measure the roc curve of the model.\n\nInstruction:\nWhat is the purpose of the auc metric?\n\nResponse:\nThe auc metric is used to measure the auc of the model.\n\nInstruction:\nWhat is the purpose of the roc curve metric?\n\nResponse:\nThe roc curve metric is used to measure the roc curve of the model.\n\nInstruction:\n\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"Explain the purpose of feature correlation so that non-specialist wil understand.\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T14:21:33.965312Z","iopub.execute_input":"2024-02-26T14:21:33.966257Z","iopub.status.idle":"2024-02-26T14:21:39.716742Z","shell.execute_reply.started":"2024-02-26T14:21:33.96622Z","shell.execute_reply":"2024-02-26T14:21:39.715707Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Instruction:\nExplain the purpose of feature correlation so that non-specialist wil understand.\n\nResponse:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of finding the relationship between two or more features.\n\nExplanation:\nFeature correlation is the process of\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**As you can see the responses are quite poor and uncomprehensive :(**","metadata":{}},{"cell_type":"markdown","source":"# LoRa enabeling","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 4.\ngemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:28:48.400814Z","iopub.execute_input":"2024-02-26T20:28:48.401231Z","iopub.status.idle":"2024-02-26T20:28:48.869133Z","shell.execute_reply.started":"2024-02-26T20:28:48.401193Z","shell.execute_reply":"2024-02-26T20:28:48.868234Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Fine tuning the model","metadata":{}},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(data, epochs=1, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:29:00.616716Z","iopub.execute_input":"2024-02-26T20:29:00.617087Z","iopub.status.idle":"2024-02-26T20:41:34.017533Z","shell.execute_reply.started":"2024-02-26T20:29:00.617058Z","shell.execute_reply":"2024-02-26T20:41:34.016545Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m751s\u001b[0m 730ms/step - loss: 0.4588 - sparse_categorical_accuracy: 0.5242\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7bab1f970790>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Testing same prompts","metadata":{}},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"Explain the purpose of feature correlation so that non-specialist wil understand.\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T14:55:06.061844Z","iopub.execute_input":"2024-02-26T14:55:06.062301Z","iopub.status.idle":"2024-02-26T14:55:21.203365Z","shell.execute_reply.started":"2024-02-26T14:55:06.062266Z","shell.execute_reply":"2024-02-26T14:55:21.202371Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Instruction:\nExplain the purpose of feature correlation so that non-specialist wil understand.\n\nResponse:\nA feature correlation is an algorithm that is used in machine learning to determine if there is any correlation between two different variables. This can be useful in identifying relationships between two variables that are not immediately obvious. For example, if one variable is used as a predictor for predicting another variable, a correlation can be found between the two variables if there is a strong correlation between them. This can help to improve the accuracy of predictions.\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = template.format(\n    instruction=\"What is the purpose of the accuracy metric?\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T14:56:04.370254Z","iopub.execute_input":"2024-02-26T14:56:04.371023Z","iopub.status.idle":"2024-02-26T14:56:05.793874Z","shell.execute_reply.started":"2024-02-26T14:56:04.370989Z","shell.execute_reply":"2024-02-26T14:56:05.792957Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Instruction:\nWhat is the purpose of the accuracy metric?\n\nResponse:\nThe accuracy metric tells you how often a model is correct. It is a very simple metric and can be calculated using a simple if statement. Accuracy is calculated as the number of correct predictions divided by the number of predictions.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nprompt = template.format(\n    instruction=\"Write down a simple python code for loading a dataset into Kaggle notebook.\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=512))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:47:03.968352Z","iopub.execute_input":"2024-02-26T20:47:03.969365Z","iopub.status.idle":"2024-02-26T20:47:18.293267Z","shell.execute_reply.started":"2024-02-26T20:47:03.969331Z","shell.execute_reply":"2024-02-26T20:47:18.292245Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Instruction:\nWrite down a simple python code for loading a dataset into Kaggle notebook.\n\nResponse:\nYou can use pandas.read_csv() to read the data from a csv file. The below code is an example:\nimport pandas as pd\ndata = pd.read_csv('/content/drive/My Drive/data.csv')\ndata\ndata.head()\nCPU times: user 15.7 s, sys: 84.9 ms, total: 15.8 s\nWall time: 14.3 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nprompt = template.format(\n    instruction=\"Write down a code for defining a simple CNN in pytorch from scratch, include image processing.\",\n    response=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=512))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T20:57:11.050741Z","iopub.execute_input":"2024-02-26T20:57:11.05157Z","iopub.status.idle":"2024-02-26T20:57:11.551256Z","shell.execute_reply.started":"2024-02-26T20:57:11.051526Z","shell.execute_reply":"2024-02-26T20:57:11.550282Z"},"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m<timed exec>:5\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: GenerativeTask.generate() got an unexpected keyword argument 'temperature'"],"ename":"TypeError","evalue":"GenerativeTask.generate() got an unexpected keyword argument 'temperature'","output_type":"error"}]},{"cell_type":"markdown","source":"## Preparing data for fine tuning for code generation purposes","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndata = []\ni = 0\nwith open('/kaggle/input/python-code-instruction-dataset/train.csv') as file:\n    features = pd.read_csv(file)\n    for i, row in features.iterrows():\n        # Filter out examples with context, to keep it simple.\n#         if features[\"context\"]:\n#             continue\n        # Format the entire example as a single string.\n        template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{output}\"\n        data.append(template.format(**row))\n\n# Only use 1000 training examples, to keep it fast.\ndata = data[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:15:25.994055Z","iopub.execute_input":"2024-02-26T21:15:25.994478Z","iopub.status.idle":"2024-02-26T21:15:27.678582Z","shell.execute_reply.started":"2024-02-26T21:15:25.994444Z","shell.execute_reply":"2024-02-26T21:15:27.67776Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 512\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(data, epochs=1, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:17:39.201691Z","iopub.execute_input":"2024-02-26T21:17:39.202121Z","iopub.status.idle":"2024-02-26T21:30:07.166609Z","shell.execute_reply.started":"2024-02-26T21:17:39.202091Z","shell.execute_reply":"2024-02-26T21:30:07.165645Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 730ms/step - loss: 0.2402 - sparse_categorical_accuracy: 0.7908\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7bab20d9e3e0>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\nprompt = template.format(\n    instruction=\"Write down a code for defining a simple CNN in pytorch from scratch, include image processing.\",\n    output=\"\",\n)\nprint(gemma_lm.generate(prompt, max_length=512))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:38:12.216871Z","iopub.execute_input":"2024-02-26T21:38:12.217661Z","iopub.status.idle":"2024-02-26T21:38:35.384577Z","shell.execute_reply.started":"2024-02-26T21:38:12.217626Z","shell.execute_reply":"2024-02-26T21:38:35.383548Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Instruction:\nWrite down a code for defining a simple CNN in pytorch from scratch, include image processing.\n\nResponse:\nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision\n\n# Define model parameters\ninput_channels = 3\ninput_shape = [1, input_channels, 32, 32]\nn_classes = 1\n\n# Define network architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(input_channels, 20, 5, padding=2)\n        self.conv2 = nn.Conv2d(20, 50, 5, padding=2)\n        self.pool = nn.MaxPool2d(2, 2)\n\n    def forward(self, x):\n        x = self.pool(self.conv1(x.reshape(-1, input_channels, 32, 32)))\n        x = self.conv2(x.reshape(-1, 20 * 5 * 5))\n        x = x.view(-1, 50 * 5 * 5)\n\n        # Add softmax layer\n        return nn.Softmax(dim=1)(x)\n\n# Initialize the network\nnet = Net()\n\n# Load the model parameters\nos.makedirs('saved_model', exist_ok=True)\nnet.load_state_dict(torch.load('saved_model/saved_net.model'))\n\n# Predict on a single image\ninput = torch.randn(1, input_channels, 32, 32)\noutput = net(input)\n\nprint(f'Output shape: {output.shape}')\nprint(f'Class probabilities: {output}')\n\n# Save the model\ntorch.save(net.state_dict(), 'saved_net.model')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Planning to use some Hugging Face datasets further, stay tuned <3**","metadata":{}},{"cell_type":"code","source":"# package details\n!pip show datasets","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:53:55.427506Z","iopub.execute_input":"2024-02-26T21:53:55.427953Z","iopub.status.idle":"2024-02-26T21:54:07.657371Z","shell.execute_reply.started":"2024-02-26T21:53:55.427919Z","shell.execute_reply":"2024-02-26T21:54:07.656113Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Name: datasets\nVersion: 2.1.0\nSummary: HuggingFace community-driven open-source library of datasets\nHome-page: https://github.com/huggingface/datasets\nAuthor: HuggingFace Inc.\nAuthor-email: thomas@huggingface.co\nLicense: Apache 2.0\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: aiohttp, dill, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, requests, responses, tqdm, xxhash\nRequired-by: \n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-02-26T21:54:13.273467Z","iopub.execute_input":"2024-02-26T21:54:13.274345Z","iopub.status.idle":"2024-02-26T21:54:13.713339Z","shell.execute_reply.started":"2024-02-26T21:54:13.274304Z","shell.execute_reply":"2024-02-26T21:54:13.712481Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# from kaggle_secrets import UserSecretsClient\n# from huggingface_hub import login\n\n# user_secrets = UserSecretsClient()\n# hf_token = user_secrets.get_secret(\"Kaggle\")\n\n# # Login to Hugging Face\n# login(hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T15:33:30.666795Z","iopub.execute_input":"2024-02-26T15:33:30.667622Z","iopub.status.idle":"2024-02-26T15:33:31.02846Z","shell.execute_reply.started":"2024-02-26T15:33:30.66759Z","shell.execute_reply":"2024-02-26T15:33:31.027416Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"data = load_dataset(\n    \"iamtarun/python_code_instructions_18k_alpaca\" \n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}